### 从图灵测试到ChatGPT
**图灵测试：**
如果一个人（代号C）使用测试对象皆理解的语言去询问两个他不能看见的对象任意一串问题，其中一个是正常思维的人（代号B），另一个是机器（代号A）。如果经过若干询问以后，C不能得出实质的区别来分辨A与B的不同，则此机器A通过图灵测试

**自然语言处理** 是计算机科学、人工智能和语言学的交叉领域，关注计算机和人类语言之间的相互作用。常见的任务和应用包括：
- 信息抽取
- 文本分类
- 文本摘要
- 机器翻译
- 问答系统
- 对话系统
- 等等

图灵测试与自然语言处理任务有着密切而复杂的关系：
- 图灵测试是自然语言处理任务的一个重要驱动力
- 图灵测试是自然语言处理任务的一个重要目标

### 语言模型基础
**计算机如何识别句子：**
首先把一句话分成一个一个**token**，即将句子碎片化。例如把一个句子碎片成一个一个的字去理解，或者把句子碎片成一个一个的词组。
英文常见是把一句话分成**子词**。子词把不在词表里的词或不常见的词拆成比较常见的片段。优点：同时兼顾词表大小和语义表示。
中文常见是把一句话分成**字+词**。

用token表示句子，用数字表示token -> 用数字表示句子

定义一个大型字典，将词用向量线性表示：[0,0,0,0,1,0,0,0,0...]，
一句话就是一个二维线性向量。

**语言模型：**
给定一个句子，预测下一个词出现的概率。


### Transformer
Transformer 是一个基于注意力机制的编码器-解码器（Encoder-Decoder）架构。从编码器输入，从解码器输出。
来自 Google2017 年发的一篇论文：“Attention Is All You Need”，其最重要的核心就是提出来的自注意力（Self-Attention）机制。简单来说，就是在语言模型建模过程中，把注意力放在那些重要的Token上。